{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a classification problem, we generally use a softmax function to get the predicted score. Although a softmax function can normalize the results, it cannot be used to identify unknown class samples and hard samples. A low predict score for a certain class does not mean an unknown class or a hard sample. In addition, it is hard to understand which part of images have a strong influence on the final prediction, especially for these hard samples. So, I used Gradient-weighted Class Activation Mapping (Grad-CAM) to explain the results.\n",
    "\n",
    "There are two contributions in this final project. First, I proposed a robust classification approach to identify samples from unknown classes and hard samples. Second, I used Grad-CAM to visualization the attention of neural networks to make the results more explainable. Specifically, I find that apparent misclassifications tend to have a larger attention area and understandable misclassifications tend to have a smaller attention are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the requirement of page numbers, I just include the code for training. Other codes can be found in [github](https://github.com/xiaoyuxie-vico/ExplainableAI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cat and dog dataset used in this project is downloaded from [Kaggel](https://www.kaggle.com/tongpython/cat-and-dog). Several images are shown in the below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data augmentation in the training set includes random rotation (20), random crop scale (0.8, 1.0), Random Horizontal Flip, Random Affine, Normalization. The test set does not use data augmentation.\n",
    "\n",
    "The batch size is 32 and the dataset will be shuffled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model is ResNet50 and change the FC layer\n",
    "model = torchvision.models.resnet50(pretrained=True).to(device)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048,2),\n",
    ").to(device)\n",
    "# define loss and optimizer\n",
    "criterian = nn.CrossEntropyLoss()\n",
    "optimizers = torch.optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3631159b1104ef39543da03cbe2b245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c225f65c28cd471ea77567d0a68d1690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_idx: 0, loss: 0.8079, acc: 0.375\n",
      "epoch: 0, batch_idx: 20, loss: 0.2993, acc: 0.6994\n",
      "epoch: 0, batch_idx: 40, loss: 0.1262, acc: 0.83\n",
      "epoch: 0, batch_idx: 60, loss: 0.0862, acc: 0.8678\n",
      "epoch: 0, batch_idx: 80, loss: 0.1332, acc: 0.88\n",
      "epoch: 0, batch_idx: 100, loss: 0.1579, acc: 0.8936\n",
      "epoch: 0, batch_idx: 120, loss: 0.0551, acc: 0.9062\n",
      "epoch: 0, batch_idx: 140, loss: 0.0701, acc: 0.9158\n",
      "epoch: 0, batch_idx: 160, loss: 0.0389, acc: 0.9216\n",
      "epoch: 0, batch_idx: 180, loss: 0.2492, acc: 0.9252\n",
      "epoch: 0, batch_idx: 200, loss: 0.0473, acc: 0.93\n",
      "epoch: 0, batch_idx: 220, loss: 0.3588, acc: 0.9328\n",
      "epoch: 0, batch_idx: 240, loss: 0.0424, acc: 0.9354\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4a7dc01a994a0a97fe9002d13db656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch_idx: 0, loss: 0.059, acc: 0.9688\n",
      "epoch: 1, batch_idx: 20, loss: 0.124, acc: 0.9598\n",
      "epoch: 1, batch_idx: 40, loss: 0.0653, acc: 0.9627\n",
      "epoch: 1, batch_idx: 60, loss: 0.1109, acc: 0.9606\n",
      "epoch: 1, batch_idx: 80, loss: 0.0398, acc: 0.9633\n",
      "epoch: 1, batch_idx: 100, loss: 0.0803, acc: 0.965\n",
      "epoch: 1, batch_idx: 120, loss: 0.1241, acc: 0.9636\n",
      "epoch: 1, batch_idx: 140, loss: 0.1066, acc: 0.9645\n",
      "epoch: 1, batch_idx: 160, loss: 0.0354, acc: 0.9658\n",
      "epoch: 1, batch_idx: 180, loss: 0.0665, acc: 0.967\n",
      "epoch: 1, batch_idx: 200, loss: 0.038, acc: 0.9674\n",
      "epoch: 1, batch_idx: 220, loss: 0.1271, acc: 0.9675\n",
      "epoch: 1, batch_idx: 240, loss: 0.0412, acc: 0.9676\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d885aa01c781481e935293ac00470b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, batch_idx: 0, loss: 0.1402, acc: 0.9688\n",
      "epoch: 2, batch_idx: 20, loss: 0.0531, acc: 0.9464\n",
      "epoch: 2, batch_idx: 40, loss: 0.0554, acc: 0.952\n",
      "epoch: 2, batch_idx: 60, loss: 0.049, acc: 0.96\n",
      "epoch: 2, batch_idx: 80, loss: 0.0165, acc: 0.9599\n",
      "epoch: 2, batch_idx: 100, loss: 0.0787, acc: 0.9592\n",
      "epoch: 2, batch_idx: 120, loss: 0.2566, acc: 0.9592\n",
      "epoch: 2, batch_idx: 140, loss: 0.0621, acc: 0.9603\n",
      "epoch: 2, batch_idx: 160, loss: 0.089, acc: 0.9622\n",
      "epoch: 2, batch_idx: 180, loss: 0.0474, acc: 0.9624\n",
      "epoch: 2, batch_idx: 200, loss: 0.0453, acc: 0.963\n",
      "epoch: 2, batch_idx: 220, loss: 0.1472, acc: 0.9632\n",
      "epoch: 2, batch_idx: 240, loss: 0.0244, acc: 0.9629\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facb61dbd957421ebdd469b39575aebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, batch_idx: 0, loss: 0.0847, acc: 0.9375\n",
      "epoch: 3, batch_idx: 20, loss: 0.1979, acc: 0.9673\n",
      "epoch: 3, batch_idx: 40, loss: 0.0223, acc: 0.9665\n",
      "epoch: 3, batch_idx: 60, loss: 0.0785, acc: 0.9672\n",
      "epoch: 3, batch_idx: 80, loss: 0.2462, acc: 0.9699\n",
      "epoch: 3, batch_idx: 100, loss: 0.1015, acc: 0.9712\n",
      "epoch: 3, batch_idx: 120, loss: 0.0163, acc: 0.9698\n",
      "epoch: 3, batch_idx: 140, loss: 0.0351, acc: 0.9699\n",
      "epoch: 3, batch_idx: 160, loss: 0.0968, acc: 0.9697\n",
      "epoch: 3, batch_idx: 180, loss: 0.1516, acc: 0.9701\n",
      "epoch: 3, batch_idx: 200, loss: 0.0669, acc: 0.9703\n",
      "epoch: 3, batch_idx: 220, loss: 0.105, acc: 0.9692\n",
      "epoch: 3, batch_idx: 240, loss: 0.0096, acc: 0.9695\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eaeb53f1ea74020b3d7289d0c1bc5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, batch_idx: 0, loss: 0.0254, acc: 1.0\n",
      "epoch: 4, batch_idx: 20, loss: 0.1286, acc: 0.9524\n",
      "epoch: 4, batch_idx: 40, loss: 0.0283, acc: 0.9566\n",
      "epoch: 4, batch_idx: 60, loss: 0.0803, acc: 0.9621\n",
      "epoch: 4, batch_idx: 80, loss: 0.1678, acc: 0.9603\n",
      "epoch: 4, batch_idx: 100, loss: 0.0262, acc: 0.9629\n",
      "epoch: 4, batch_idx: 120, loss: 0.1569, acc: 0.962\n",
      "epoch: 4, batch_idx: 140, loss: 0.1616, acc: 0.9637\n",
      "epoch: 4, batch_idx: 160, loss: 0.0036, acc: 0.9651\n",
      "epoch: 4, batch_idx: 180, loss: 0.1294, acc: 0.9639\n",
      "epoch: 4, batch_idx: 200, loss: 0.078, acc: 0.9628\n",
      "epoch: 4, batch_idx: 220, loss: 0.013, acc: 0.9638\n",
      "epoch: 4, batch_idx: 240, loss: 0.0285, acc: 0.965\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fec7d5cf1346c6ad252dfce38185d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, batch_idx: 0, loss: 0.1092, acc: 0.9688\n",
      "epoch: 5, batch_idx: 20, loss: 0.0683, acc: 0.9643\n",
      "epoch: 5, batch_idx: 40, loss: 0.0342, acc: 0.9611\n",
      "epoch: 5, batch_idx: 60, loss: 0.1717, acc: 0.9636\n",
      "epoch: 5, batch_idx: 80, loss: 0.2233, acc: 0.9649\n",
      "epoch: 5, batch_idx: 100, loss: 0.0068, acc: 0.9669\n",
      "epoch: 5, batch_idx: 120, loss: 0.23, acc: 0.969\n",
      "epoch: 5, batch_idx: 140, loss: 0.0431, acc: 0.9696\n",
      "epoch: 5, batch_idx: 160, loss: 0.0468, acc: 0.9699\n",
      "epoch: 5, batch_idx: 180, loss: 0.0518, acc: 0.9693\n",
      "epoch: 5, batch_idx: 200, loss: 0.2472, acc: 0.9663\n",
      "epoch: 5, batch_idx: 220, loss: 0.0112, acc: 0.9658\n",
      "epoch: 5, batch_idx: 240, loss: 0.0219, acc: 0.9667\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ba61bba73544eda6c8a9ff6982952f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, batch_idx: 0, loss: 0.0056, acc: 1.0\n",
      "epoch: 6, batch_idx: 20, loss: 0.0128, acc: 0.9792\n",
      "epoch: 6, batch_idx: 40, loss: 0.0302, acc: 0.9779\n",
      "epoch: 6, batch_idx: 60, loss: 0.2222, acc: 0.9744\n",
      "epoch: 6, batch_idx: 80, loss: 0.0407, acc: 0.9691\n",
      "epoch: 6, batch_idx: 100, loss: 0.0881, acc: 0.9703\n",
      "epoch: 6, batch_idx: 120, loss: 0.0042, acc: 0.9698\n",
      "epoch: 6, batch_idx: 140, loss: 0.1397, acc: 0.9703\n",
      "epoch: 6, batch_idx: 160, loss: 0.0093, acc: 0.9695\n",
      "epoch: 6, batch_idx: 180, loss: 0.035, acc: 0.9706\n",
      "epoch: 6, batch_idx: 200, loss: 0.0567, acc: 0.9708\n",
      "epoch: 6, batch_idx: 220, loss: 0.0137, acc: 0.9717\n",
      "epoch: 6, batch_idx: 240, loss: 0.0499, acc: 0.9712\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa02729400649b1a032e23d97b34c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, batch_idx: 0, loss: 0.2208, acc: 0.875\n",
      "epoch: 7, batch_idx: 20, loss: 0.0061, acc: 0.9673\n",
      "epoch: 7, batch_idx: 40, loss: 0.0951, acc: 0.9649\n",
      "epoch: 7, batch_idx: 60, loss: 0.0329, acc: 0.9652\n",
      "epoch: 7, batch_idx: 80, loss: 0.0123, acc: 0.966\n",
      "epoch: 7, batch_idx: 100, loss: 0.0222, acc: 0.9657\n",
      "epoch: 7, batch_idx: 120, loss: 0.005, acc: 0.9675\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] training')\n",
    "n_epochs = 20\n",
    "\n",
    "loss_history, acc_history = [], []\n",
    "for epoch in tqdm_notebook(range(n_epochs)):\n",
    "    model.train()\n",
    "    correct, total = 0., 0.\n",
    "    for batch_idx, (data, labels, _) in tqdm_notebook(enumerate(train_dataloader)):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizers.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterian(output,labels)\n",
    "        loss.backward()\n",
    "        optimizers.step()\n",
    "        \n",
    "        pred_scores = F.softmax(output, dim=1)\n",
    "        pred_labels = torch.max(pred_scores, axis=1)[1]\n",
    "        correct += (pred_labels==labels.long()).sum().item()\n",
    "        total += labels.shape[0]\n",
    "        acc = round(float(correct)/total, 4)\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        acc_history.append(acc)\n",
    "            \n",
    "        if batch_idx % 20 == 0:\n",
    "            print('epoch: {}, batch_idx: {}, loss: {}, acc: {}'.format(\n",
    "                epoch, batch_idx, round(loss.item(), 4), acc))\n",
    "    if acc > 0.95:\n",
    "        torch.save(model.state_dict(),'./models/resnet50-epoch{}-Acc{}.h5'.format(epoch, int(acc*10000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and accuracy in the training set are:\n",
    "<img src='figures/loss.jpg' width=\"50%\">\n",
    "**Finally, the accuracy in the training set and the test set are 0.9809 and 0.9812 respectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis (test set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althought the accuracy is high (more than 0.98), the trained model still can make some errors.\n",
    "\n",
    "True label is cat, but predicted label is dog and the score is high.\n",
    "<img src='figures/error_cat.jpg' width=\"35%\">\n",
    "\n",
    "True label is dog, but predicted label is cat and the score is high.\n",
    "<img src='figures/error_dog.jpg' width=\"70%\">\n",
    "\n",
    "The Grad-CAM results for these wrong predicted images are:\n",
    "<img src='figures/cam.jpg' width=\"70%\">\n",
    "\n",
    "We can find that: (1) Apparent misclassifications tend to have a larger attention area; (2) Understandable misclassifications tend to have a smaller attention area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem about apparent misclassifications and reduce the number of understandable misclassifications, we want to analyze the distribution of logits and scores for different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we analyzed the distribution of logits for cat and dog class. The results show that the absolute value for most of logits are in [2, 7], which means that in our training set it is rarely for the model to make a prediction with a lower logits (around 0) or a higher logit (greater than 7 or less than -7). Thus, it is unresonable to believe the prediction if model give such logits. This is the key observation in this project.\n",
    "\n",
    "<img src='figures/logits.jpg' width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel density estimation for logits is:\n",
    "<img src='figures/logits_density.jpg' width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the robustness of classification using logits kernel density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common and unavoidable for a model to predict some unkonwn images, including images from unknown classes. Thus, we downloaded three kind of images including a human face, landscape, and apples, which are shown in the below. All of these classes are not included in the training set. We want our model give a low score for these images. But we can find that for the first and third image the model give a high score, which means the model make some serious misclassifications.\n",
    "\n",
    "For the fourth and fiveth image, even though they are also come from the Internet, the model can give a good prediciton and high scores. Thus, the trained model have a good generalization.\n",
    "\n",
    "If we see the average of density, we can find that if we use a thershold of 0.04, these wrong predicitons can be alleviated. This is because our model has not \"seen\" these classes, it will give a low logits for these images. Then we can use this conclusion to identify the unseen classes images and improve the robustness of the model.\n",
    "\n",
    "<img src='figures/out_of_dataset.jpg' width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we test some images from the Internet. Now, we will analyze the test set. The results are shown below. These results also show that a threshold of 0.04 for the average of density is good enough to elimate the wrong predictions.\n",
    "\n",
    "<img src='figures/out_of_dataset_1.jpg' width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The proposed image classification approach can identify data out of training set based on logits kernel density estimation;\n",
    "- The proposed image classification approach can help to improve accuracy by identifying data with low density;\n",
    "- The proposed approach is explainable and can be understand visually;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
